{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "glirRc6O-_fq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9275e77-ee31-4d0d-be63-a5689c9db6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Nightingale Memory MVP Pipeline in Colab ---\n",
            "\n",
            "Processed 6 sentences.\n",
            "\n",
            "--- Task Metric Test Summary ---\n",
            "\n",
            "[Task 2: Intent Gate]\n",
            "  Accuracy: 100.00% (6/6)\n",
            "  p95 Latency: 0.009528 ms\n",
            "\n",
            "[Task 3: Retrieval/Storage]\n",
            "  Hit Rate: 83.33% (5/6)\n",
            "  p95 Latency: 0.001421 ms\n",
            "\n",
            "[Task 5: Provenance/Topic]\n",
            "  Topic Hit Rate: 83.33% (5/6)\n",
            "  p95 Latency: 0.007424 ms\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "âœ… Script successfully validated in Google Colab.\n"
          ]
        }
      ],
      "source": [
        "# memory_mvp_pipeline_colab.py\n",
        "# A minimal, local prototype for Nightingale's Memory MVP (T2, T3, T5).\n",
        "# This script is validated to run in Google Colab, uses only the Python\n",
        "# standard library, reads test data from a specified JSON file, and\n",
        "# runs a simple pipeline with performance tests.\n",
        "\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import os\n",
        "\n",
        "# --- Mock Memory Store for Task 3 ---\n",
        "# A simple dictionary to simulate a memory retrieval system.\n",
        "MEMORY_STORE = {\n",
        "    \"symptom\": [\n",
        "        \"Memory: Ask about onset, duration, and severity.\",\n",
        "        \"Memory: Check for related fever or shortness of breath.\"\n",
        "    ],\n",
        "    \"appointment\": [\n",
        "        \"Memory: Check patient's availability for next week.\",\n",
        "        \"Memory: Confirm preferred location.\"\n",
        "    ],\n",
        "    \"billing\": [\n",
        "        \"Memory: Pull up the latest statement for the patient.\",\n",
        "        \"Memory: Check insurance co-pay details.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- Task 2: Intent Gate (Classifier) ---\n",
        "def classify_intent(sentence_id: str, sentence: str) -> dict:\n",
        "    \"\"\"\n",
        "    Classifies the intent of a sentence using simple rule-based string matching.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the sentence ID, intent label, a flag indicating\n",
        "        if a response is needed, and the execution latency.\n",
        "    \"\"\"\n",
        "    start_time = time.perf_counter()\n",
        "    sentence_lower = sentence.lower()\n",
        "    intent_label = \"other\"\n",
        "    needs_response = False\n",
        "\n",
        "    if any(kw in sentence_lower for kw in [\"pain\", \"headache\", \"sick\", \"nausea\"]):\n",
        "        intent_label = \"symptom\"\n",
        "        needs_response = True\n",
        "    elif any(kw in sentence_lower for kw in [\"appointment\", \"book\", \"schedule\", \"visit\"]):\n",
        "        intent_label = \"appointment\"\n",
        "        needs_response = True\n",
        "    elif any(kw in sentence_lower for kw in [\"bill\", \"incorrect\", \"charge\", \"payment\"]):\n",
        "        intent_label = \"billing\"\n",
        "        needs_response = False\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    latency_ms = (end_time - start_time) * 1000\n",
        "\n",
        "    return {\n",
        "        \"sentence_id\": sentence_id,\n",
        "        \"intent_label\": intent_label,\n",
        "        \"needs_response\": needs_response,\n",
        "        \"latency_ms\": latency_ms,\n",
        "    }\n",
        "\n",
        "# --- Task 3: Retrieval / Storage (Mock) ---\n",
        "def retrieve_memory(sentence_id: str, intent_label: str) -> dict:\n",
        "    \"\"\"\n",
        "    Mocks a retrieval step by looking up an intent in a hardcoded dictionary.\n",
        "\n",
        "    Comment: This is a placeholder for a more advanced retrieval system.\n",
        "    Future paths to investigate:\n",
        "    - Path-A: Use text embeddings (e.g., from Sentence-BERT) for semantic search\n",
        "      in a vector database. This provides flexibility but adds a dependency.\n",
        "    - Path-B: Reuse embeddings from an ASR encoder. This could be more efficient\n",
        "      by avoiding a separate embedding step but might be less semantically rich.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the sentence ID, a list of retrieved items,\n",
        "        a hit flag, and the execution latency.\n",
        "    \"\"\"\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    retrieved_items = MEMORY_STORE.get(intent_label, [])\n",
        "    hit = intent_label in MEMORY_STORE\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    latency_ms = (end_time - start_time) * 1000\n",
        "\n",
        "    return {\n",
        "        \"sentence_id\": sentence_id,\n",
        "        \"retrieved_items\": retrieved_items,\n",
        "        \"hit\": hit,\n",
        "        \"latency_ms\": latency_ms,\n",
        "    }\n",
        "\n",
        "# --- Task 5: Provenance / Topic (Dictionary) ---\n",
        "TOPIC_DICTIONARY = {\n",
        "    \"symptom\": [\"pain\", \"headache\", \"fever\", \"nausea\", \"medication\"],\n",
        "    \"scheduling\": [\"schedule\", \"book\", \"visit\", \"hours\"],\n",
        "    \"payment\": [\"bill\", \"payment\", \"invoice\", \"charge\", \"incorrect\"],\n",
        "    \"followup\": [\"follow-up\"],\n",
        "}\n",
        "\n",
        "def assign_topic(sentence_id: str, sentence: str) -> dict:\n",
        "    \"\"\"\n",
        "    Assigns a topic label using a fixed dictionary and records provenance.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the chunk ID (same as sentence ID here),\n",
        "        the topic label, a list of source sentence IDs for provenance,\n",
        "        and the execution latency.\n",
        "    \"\"\"\n",
        "    start_time = time.perf_counter()\n",
        "    sentence_lower = sentence.lower()\n",
        "    topic_label = \"general\" # Default topic\n",
        "\n",
        "    # Check for specific 'follow-up' topic first as 'appointment' keywords might also be present\n",
        "    if \"follow-up\" in sentence_lower:\n",
        "        topic_label = \"followup\"\n",
        "    else:\n",
        "        for topic, keywords in TOPIC_DICTIONARY.items():\n",
        "            if any(kw in sentence_lower for kw in keywords):\n",
        "                topic_label = topic\n",
        "                break\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    latency_ms = (end_time - start_time) * 1000\n",
        "\n",
        "    return {\n",
        "        \"chunk_id\": sentence_id, # For this MVP, a chunk is a single sentence\n",
        "        \"topic_label\": topic_label,\n",
        "        \"provenance\": [sentence_id],\n",
        "        \"latency_ms\": latency_ms,\n",
        "    }\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def calculate_p95(latencies: list[float]) -> float:\n",
        "    \"\"\"Calculates the 95th percentile latency.\"\"\"\n",
        "    if not latencies:\n",
        "        return 0.0\n",
        "    sorted_latencies = sorted(latencies)\n",
        "    p95_index = math.ceil(len(sorted_latencies) * 0.95) - 1\n",
        "    return sorted_latencies[p95_index]\n",
        "\n",
        "def load_data(filepath: str) -> list[dict]:\n",
        "    \"\"\"Loads and validates the test data from a JSON file.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Data file not found at '{filepath}'\")\n",
        "        return []\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        # Basic validation\n",
        "        for i, record in enumerate(data):\n",
        "            if not all(k in record for k in [\"sentence_id\", \"text\", \"expected_intent\", \"expected_topic\"]):\n",
        "                print(f\"Error: Record {i} in {filepath} is missing required keys.\")\n",
        "                return []\n",
        "        return data\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from file '{filepath}'\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# --- Main Pipeline Execution ---\n",
        "def run_pipeline(sample_data: list[dict]):\n",
        "    \"\"\"\n",
        "    Runs the full T2 -> T3 -> T5 pipeline on the sample data and prints a report.\n",
        "    \"\"\"\n",
        "    if not sample_data:\n",
        "        print(\"Pipeline execution halted: No valid data loaded.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Running Nightingale Memory MVP Pipeline in Colab ---\")\n",
        "\n",
        "    t2_latencies, t3_latencies, t5_latencies = [], [], []\n",
        "    t2_correct, t3_hits, t5_correct = 0, 0, 0\n",
        "\n",
        "    for item in sample_data:\n",
        "        sentence_id, text = item[\"sentence_id\"], item[\"text\"]\n",
        "        expected_intent = item[\"expected_intent\"]\n",
        "        expected_topic = item[\"expected_topic\"]\n",
        "\n",
        "        # Task 2: Intent Gate\n",
        "        t2_output = classify_intent(sentence_id, text)\n",
        "        t2_latencies.append(t2_output[\"latency_ms\"])\n",
        "        if t2_output[\"intent_label\"] == expected_intent:\n",
        "            t2_correct += 1\n",
        "\n",
        "        # Task 3: Retrieval\n",
        "        t3_output = retrieve_memory(sentence_id, t2_output[\"intent_label\"])\n",
        "        t3_latencies.append(t3_output[\"latency_ms\"])\n",
        "        if t3_output[\"hit\"]:\n",
        "            t3_hits += 1\n",
        "\n",
        "        # Task 5: Topic Assignment\n",
        "        t5_output = assign_topic(sentence_id, text)\n",
        "        t5_latencies.append(t5_output[\"latency_ms\"])\n",
        "        if t5_output[\"topic_label\"] == expected_topic:\n",
        "            t5_correct += 1\n",
        "\n",
        "    print(f\"\\nProcessed {len(sample_data)} sentences.\\n\")\n",
        "\n",
        "    # --- Calculate and Print Metrics ---\n",
        "    num_records = len(sample_data)\n",
        "    t2_accuracy = t2_correct / num_records\n",
        "    t3_hit_rate = t3_hits / num_records\n",
        "    t5_hit_rate = t5_correct / num_records\n",
        "\n",
        "    t2_p95_latency = calculate_p95(t2_latencies)\n",
        "    t3_p95_latency = calculate_p95(t3_latencies)\n",
        "    t5_p95_latency = calculate_p95(t5_latencies)\n",
        "\n",
        "    # --- Final Report ---\n",
        "    print(\"--- Task Metric Test Summary ---\")\n",
        "    print(\"\\n[Task 2: Intent Gate]\")\n",
        "    print(f\"  Accuracy: {t2_accuracy:.2%} ({t2_correct}/{num_records})\")\n",
        "    print(f\"  p95 Latency: {t2_p95_latency:.6f} ms\")\n",
        "\n",
        "    print(\"\\n[Task 3: Retrieval/Storage]\")\n",
        "    print(f\"  Hit Rate: {t3_hit_rate:.2%} ({t3_hits}/{num_records})\")\n",
        "    print(f\"  p95 Latency: {t3_p95_latency:.6f} ms\")\n",
        "\n",
        "    print(\"\\n[Task 5: Provenance/Topic]\")\n",
        "    print(f\"  Topic Hit Rate: {t5_hit_rate:.2%} ({t5_correct}/{num_records})\")\n",
        "    print(f\"  p95 Latency: {t5_p95_latency:.6f} ms\")\n",
        "    print(\"\\n----------------------------------\\n\")\n",
        "    print(\"âœ… Script successfully validated in Google Colab.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the explicit path for the Colab environment\n",
        "    DATA_PATH = \"/content/Nightingale/sample_dialogues.json\"\n",
        "    dialogue_data = load_data(DATA_PATH)\n",
        "    run_pipeline(dialogue_data)"
      ]
    }
  ]
}