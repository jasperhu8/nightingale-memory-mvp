# Nightingale Memory MVP – Minimal Prototype (Validated for Google Colab)

## Objective
This prototype implements a minimal pipeline for core tasks in Nightingale's Memory system: Task 2 (Intent Gate), Task 3 (Retrieval/Storage), and Task 5 (Provenance/Topic). The primary goal is to create a fast, deterministic, and easily understandable baseline that is validated to run end-to-end in a standard Google Colab environment without external dependencies. This allows us to test the fundamental logic and measure baseline latency in a reproducible cloud setting.

## Assumptions
The implementation adheres to several strict constraints:
* **Single Language:** The pipeline is designed for English input only.
* **No LLMs / ML Models:** All logic is rule-based or dictionary-based.
* **No Training:** The system is static and does not require a training phase.
* **Standard Library Only:** The script uses only built-in Python libraries to ensure maximum portability and zero installation friction in Colab.

## Key Decisions
* **Rule-based Classifier (T2):** A simple keyword-matching classifier was chosen for its extreme speed, predictability, and ease of debugging. This provides a solid latency baseline before introducing a more nuanced but slower ML model.
* **Dictionary Retrieval (T3):** Using a Python dictionary to mock the memory store is the simplest way to simulate a fast key-value lookup. It stands in for a future vector database, allowing us to build the surrounding application logic without the dependency.
* **Fixed Topic Dictionary (T5):** A dictionary mapping keywords to topics is deterministic and computationally trivial. It avoids the complexity and non-determinism of models like LDA or NMF, which are overkill for this initial prototype.
* **Reproducibility in Colab:** Using explicit, hardcoded file paths (e.g., `/content/Nightingale/...`) ensures the script runs without modification in any standard Colab instance once the data and script are in place.

## Task Metric Tests (Summary)
The following results were generated by running the `memory_mvp_pipeline_colab.py` script on the `sample_dialogues.json` dataset within a Google Colab environment.

| Task                      | Metric         | Score           | p95 Latency (ms) |
| :------------------------ | :------------- | :-------------- | :--------------- |
| **Task 2: Intent Gate** | Accuracy       | 100.00% (6/6)   | `0.009528`       |
| **Task 3: Retrieval** | Hit Rate       | 83.33% (5/6)    | `0.001421`       |
| **Task 5: Topic/Provenance**| Topic Hit Rate | 83.33% (5/6)    | `0.007424`       |

*(Note: Latency figures may vary slightly between Colab execution environments.)*

## Post-mortem
* **What failed first / what we cut:**
    * **Multilingual Support:** Auto-detection of language was cut to adhere to the single-language constraint.
    * **Real Vector DB/Embeddings:** We avoided `numpy`, `faiss`, or other libraries to maintain the "standard library only" rule.
    * **Probabilistic Topic Models (LDA/NMF):** These were cut as they require third-party libraries and introduce non-determinism.

* **What worked well:**
    * The deterministic, rule-based flow is exceptionally fast and predictable, even in a shared cloud environment like Colab.
    * The separation of tasks into distinct functions makes the code clean and easy to reason about.
    * The script is fully self-contained and runs without any manual setup once files are in the correct location.

* **Where stuck / remaining gaps:**
    * **Semantic Nuance:** The rule-based system cannot handle semantic similarity (e.g., "my chest hurts" vs. "chest pain"). This is the primary gap that embeddings will fill.
    * **Memory Graph:** The current retrieval is a simple key-value lookup. A true memory system would require a graph structure.
    * **PHI Cleaning:** There is no mechanism for detecting and redacting Protected Health Information (PHI).

* **Where cloud GPU is required:**
    * **Embedding Generation:** Generating high-quality text embeddings at scale using models like BERT is GPU-intensive.
    * **Model Fine-tuning:** If we decide to fine-tune a classifier on domain-specific data, GPUs will be necessary.
    * **ASR Inference:** Running large, state-of-the-art ASR models for transcription is a GPU-accelerated task.

* **Clinic/cloud deployment issues foreseen:**
    * **Latency Variance:** Network overhead and real ML models will introduce significant latency compared to this prototype.
    * **PHI & Compliance:** Handling PHI in a cloud environment requires strict adherence to HIPAA.
    * **Storage Scalability:** The choice of storage (flat files, key-value store, vector DB) will have major implications for cost and performance.

## How to Run
This pipeline is validated to run in a Google Colab notebook.

1.  **Prepare your Colab environment:**
    * Create a root folder named `Nightingale` in your Colab instance's file system (`/content/`).
    * Place the script and data file into this folder, ensuring the paths are exactly:
        * `/content/Nightingale/memory_mvp_pipeline_colab.py`
        * `/content/Nightingale/sample_dialogues.json`

2.  **Execute the script:**
    * In a new Colab cell, run the following command:
    ```sh
    !python /content/Nightingale/memory_mvp_pipeline_colab.py
    ```

3.  **Check the output:**
    * The script will execute the pipeline and print the final performance report directly to the cell's output, matching the results in the "Task Metric Tests" section above.

---

✅ Script successfully validated in Google Colab.
